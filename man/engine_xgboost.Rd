% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/engine_xgboost.R
\name{engine_xgboost}
\alias{engine_xgboost}
\title{Engine for extreme gradient boosting (XGBoost)}
\usage{
engine_xgboost(
  x,
  booster = "gbtree",
  iter = 8000L,
  learning_rate = 0.001,
  gamma = 6,
  reg_lambda = 0,
  reg_alpha = 0,
  max_depth = 2,
  subsample = 0.75,
  colsample_bytree = 0.4,
  min_child_weight = 3,
  nthread = getOption("ibis.nthread"),
  ...
)
}
\arguments{
\item{x}{\code{\link[=distribution]{distribution()}} (i.e. \code{\linkS4class{BiodiversityDistribution}}) object.}

\item{booster}{A \code{\link{character}} of the booster to use. Either \code{"gbtree"}
or \code{"gblinear"} (Default: \code{gblinear})}

\item{iter}{\code{\link{numeric}} value giving the the maximum number of boosting
iterations for cross-validation (Default: \code{8e3L}).}

\item{learning_rate}{\code{\link{numeric}} value indicating the learning rate (eta).
Lower values generally being better but also computationally more costly.
(Default: \code{1e-3})}

\item{gamma}{\code{\link{numeric}} A regularization parameter in the model. Lower
values for better estimates (Default: \code{3}). Also see
\code{"reg_lambda"} parameter for the L2 regularization on the weights}

\item{reg_lambda}{\code{\link{numeric}} L2 regularization term on weights (Default:
\code{0}).}

\item{reg_alpha}{\code{\link{numeric}} L1 regularization term on weights (Default:
\code{0}).}

\item{max_depth}{\code{\link{numeric}} The Maximum depth of a tree (Default: \code{3}).}

\item{subsample}{\code{\link{numeric}} The ratio used for subsampling to prevent
overfitting. Also used for creating a random tresting dataset (Default:
\code{0.75}).}

\item{colsample_bytree}{\code{\link{numeric}} Sub-sample ratio of columns when
constructing each tree (Default: \code{0.4}).}

\item{min_child_weight}{\code{\link{numeric}} Broadly related to the number of
instances necessary for each node (Default: \code{3}).}

\item{nthread}{\code{\link{numeric}} on the number of CPU-threads to use.}

\item{...}{Other none specified parameters.}
}
\value{
An \link{Engine}.
}
\description{
Allows to estimate eXtreme gradient descent boosting for
tree-based or linear boosting regressions. The XGBoost engine is a
flexible, yet powerful engine with many customization options, supporting
multiple options to perform single and multi-class regression and
classification tasks. For a full list of options users are advised to have
a look at the \link[xgboost:xgb.train]{xgboost::xgb.train} help file and
\url{https://xgboost.readthedocs.io}.
}
\details{
The default parameters have been set relatively conservative as to
reduce overfitting.

XGBoost supports the specification of monotonic constraints on certain
variables. Within ibis this is possible via \code{\link{XGBPrior}}. However constraints
are available only for the \code{"gbtree"} baselearners.
}
\note{
\emph{'Machine learning is statistics minus any checking of models and assumptionsâ€˜} ~ Brian D. Ripley, useR! 2004, Vienna
}
\examples{
\dontrun{
# Add xgboost as an engine
x <- distribution(background) |> engine_xgboost(iter = 4000)
}
}
\references{
\itemize{
\item Tianqi Chen and Carlos Guestrin, "XGBoost: A Scalable Tree Boosting System", 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016, https://arxiv.org/abs/1603.02754
}
}
\seealso{
\link[xgboost:xgb.train]{xgboost::xgb.train}

Other engine: 
\code{\link{engine_bart}()},
\code{\link{engine_breg}()},
\code{\link{engine_gdb}()},
\code{\link{engine_glmnet}()},
\code{\link{engine_glm}()},
\code{\link{engine_inlabru}()},
\code{\link{engine_inla}()},
\code{\link{engine_stan}()}
}
\concept{engine}
